{"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport missingno as msno\n\nfile = tf.keras.utils\nraw_df = pd.read_excel('../input/covid19/dataset.xlsx')\n\ncleaned_df = raw_df.copy()\n\n# dataprep categorical\nmask_pos_neg = {'positive': 1, 'negative': 0}\nmask_detected = {'detected': 1, 'not_detected': 0}\nmask_notdone_absent_present = {'not_done': 0, 'absent': 1, 'present': 2}\nmask_normal = {'normal': 1}\nmask_urine_color = {'light_yellow': 1, 'yellow': 2, 'citrus_yellow': 3, 'orange': 4}\nmask_urine_aspect = {'clear': 1, 'lightly_cloudy': 2, 'cloudy': 3, 'altered_coloring': 4}\nmask_realizado = {'Não Realizado': 0}\nmask_urine_leuk = {'<1000': 1000}\nmask_urine_crys = {'Ausentes': 1, 'Urato Amorfo --+': 0, 'Oxalato de Cálcio +++': 0, 'Oxalato de Cálcio -++': 0, 'Urato Amorfo +++': 0}\n\ncleaned_df = cleaned_df.replace(mask_detected)\ncleaned_df = cleaned_df.replace(mask_pos_neg)\ncleaned_df = cleaned_df.replace(mask_notdone_absent_present)\ncleaned_df = cleaned_df.replace(mask_normal)\ncleaned_df = cleaned_df.replace(mask_realizado)\ncleaned_df = cleaned_df.replace(mask_urine_leuk)\ncleaned_df = cleaned_df.replace(mask_urine_color)\ncleaned_df = cleaned_df.replace(mask_urine_aspect)\ncleaned_df = cleaned_df.replace(mask_urine_crys)\n\nnull_cleaned_df = cleaned_df.isna().mean().round(4) * 100\nnulls = null_cleaned_df[null_cleaned_df > 85]\n\ncleaned_df = cleaned_df[[col for col in cleaned_df.columns if col not in nulls]]\ncleaned_df = cleaned_df.dropna(how='any')\n\ndrop_cols = [\n    'Patient ID',\n    'Patient addmited to regular ward (1=yes, 0=no)',\n    'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n    'Patient addmited to intensive care unit (1=yes, 0=no)'\n]\n\ncleaned_df.drop(drop_cols, axis=1, inplace=True)\n\nprint('Size of the data', cleaned_df.shape)\nprint(cleaned_df.head())\n\n# Use a utility from sklearn to split and shuffle our dataset.\ntrain_df, test_df = train_test_split(cleaned_df, test_size=0.2)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2)\n\n# Form np arrays of labels and features.\ntrain_labels = np.array(train_df.pop('SARS-Cov-2 exam result'))\nbool_train_labels = train_labels != 0\nval_labels = np.array(val_df.pop('SARS-Cov-2 exam result'))\ntest_labels = np.array(test_df.pop('SARS-Cov-2 exam result'))\n\ntrain_features = np.array(train_df)\nval_features = np.array(val_df)\ntest_features = np.array(test_df)\n\nscaler = StandardScaler()\ntrain_features = scaler.fit_transform(train_features)\n\nval_features = scaler.transform(val_features)\ntest_features = scaler.transform(test_features)\n\ntrain_features = np.clip(train_features, -5, 5)\nval_features = np.clip(val_features, -5, 5)\ntest_features = np.clip(test_features, -5, 5)\n\nprint('Training labels shape:', train_labels.shape)\nprint('Validation labels shape:', val_labels.shape)\nprint('Test labels shape:', test_labels.shape)\n\nprint('Training features shape:', train_features.shape)\nprint('Validation features shape:', val_features.shape)\nprint('Test features shape:', test_features.shape)\n\nMETRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n]\n\ndef make_model(metrics = METRICS, output_bias=None):\n  if output_bias is not None:\n    output_bias = tf.keras.initializers.Constant(output_bias)\n  model = keras.Sequential([\n      keras.layers.Dense(\n          16, \n          activation='relu',\n          input_shape=(train_features.shape[-1],)\n      ),\n      keras.layers.Dropout(0.5),\n      keras.layers.Dense(\n          1, \n          activation='sigmoid',\n          bias_initializer=output_bias\n      ),\n  ])\n\n  model.compile(\n      optimizer=keras.optimizers.Adam(lr=1e-3),\n      loss=keras.losses.BinaryCrossentropy(),\n      metrics=metrics\n  )\n\n  return model\n\nEPOCHS = 10\nBATCH_SIZE = 2048\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True\n)\n\nmodel = make_model()\n\nmodel.predict(train_features[:10])\n\nresults = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\nprint(\"Loss: {:0.4f}\".format(results[0]))\n\nbaseline_history = model.fit(\n    train_features,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n    validation_data=(val_features, val_labels)\n)\n\ndef plot_metrics(history):\n  metrics =  ['loss', 'auc', 'precision', 'recall']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(2,2,n+1)\n    plt.plot(history.epoch,  history.history[metric], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    if metric == 'loss':\n      plt.ylim([0, plt.ylim()[1]])\n    elif metric == 'auc':\n      plt.ylim([0.8,1])\n    else:\n      plt.ylim([0,1])\n\n    plt.legend()\n    \nplot_metrics(baseline_history)\n\ntrain_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\ntest_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)\n\ndef plot_cm(labels, predictions, p=0.5):\n  cm = confusion_matrix(labels, predictions > p)\n  plt.figure(figsize=(5,5))\n  sns.heatmap(cm, annot=True, fmt=\"d\")\n  plt.title('Confusion matrix @{:.2f}'.format(p))\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n\nbaseline_results = model.evaluate(test_features, test_labels,\n                                  batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, baseline_results):\n  print(name, ': ', value)\nprint()\n\nplot_cm(test_labels, test_predictions_baseline)\n\n\n","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}